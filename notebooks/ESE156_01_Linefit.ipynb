{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Fitting data using Linear Least Squares \n",
    "#### (not using ready-made software tools, this version is in pure Julia (>=1.1; I am using 1.5.2 now)\n",
    "Part of ESE 156 Remote Sensing Class; 2020; \n",
    "Christian Frankenberg\n",
    "__________\n",
    "The purpose of this exercise is to use a very simple linear fit to a set of datapoints as an example for a linear forward model as well as the inversion of the parameters needed to explain the curve-fit. Once you have done this and understood the math behind it, you will be able to apply the strategy to a wider set of problems!\n",
    "\n",
    "If you have problems because some packages are not yet installed, either go into `package` mode (type `]`, you go back to main Julia prompt by hitting `delete`; hitting `;` gets you a regular console, which is nice!)\n",
    "```julia\n",
    "pkg> add Plots, Statistics, Distributions, StatsPlots, LinearAlgebra\n",
    "```\n",
    "or\n",
    "```julia\n",
    "using Pkg\n",
    "Pkg.add(\"Plots\")\n",
    "Pkg.add(\"Statistics\")\n",
    "Pkg.add(\"Distributions\")\n",
    "Pkg.add(\"StatsPlots\")\n",
    "Pkg.add(\"LinearAlgebra\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These commands make other modules available, just like `import` in python\n",
    "using Plots\n",
    "using Statistics\n",
    "using Distributions\n",
    "using StatsPlots\n",
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linefit\n",
    "Most of you (I hope so) have probably performed fits of a simple linear equation to a $(\\vec{x},\\vec{y})$ dataset. In the simplest case, we are interested in a linear fit with slope $a_2$ and offset $a_1$, in which case we can write a simple linear equation as $$y = a_1 + a_2\\cdot x\\,.$$\n",
    "\n",
    "If we have a set of $n$ points $(x_i,y_i)$, we can find the optimal coefficients $a_1,a_2$ by minimizing the squared differences between measurement and model, hence least squares. Thus, we have to find the coefficients in $a$ that minimize the cost function:\n",
    "\n",
    "$$\\chi^2 = \\sum_{i=0}^n \\left(y_i-(a_1+a_2 \\cdot x_i)\\right)^2$$\n",
    "\n",
    "To find the solution, we can rephrase the problem using linear algebra:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    " y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{n} \n",
    " \\end{bmatrix}\n",
    " =\n",
    " \\begin{bmatrix}\n",
    "  1 && x_1 \\\\\n",
    "  1 && x_2 \\\\\n",
    "  \\vdots && \\vdots \\\\\n",
    "  1 && x_n\n",
    "  \\end{bmatrix}\n",
    "  \\times\n",
    "  \\begin{bmatrix}\n",
    "  a_1 \\\\ a_2 \n",
    "  \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "In short hand, we can use $Ka=y$. The optimal solution to $a$ is obtained by minimizing the squared differences between $Ka$ and $y$, i.e. $min||y-Ka||^2$ or minimize $(y-Ka)^T (y-Ka)$, two different ways of writing it. \n",
    "\n",
    "In the ordinary unweighted least squares case, the solution is given by the normal equations (if you want the derivation beyond what we did in class, check e.g. https://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression ):\n",
    "$$a = (K^TK)^{-1}K^Ty$$\n",
    "\n",
    "*Note:* There are more stable ways to solve the normal equations than the brute force application of the matrix equations but we ignore this for now.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "### Simple example\n",
    "Here, we can use a random number generator to obtain a set of x and y points and then solve the normal equations.\n",
    "\n",
    "First off, let's explore how to access the documentation. You can just type ? in front of the command. Here, let's test what `randn` is all about:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?randn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Great, we found a random number generator for Gaussian noise, which is our assumption (most often valid due to the central limit theorem). So we can continue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True values for offset and slope:\n",
    "a1 = 0.0\n",
    "a2 = 3.0\n",
    "\n",
    "# define noise level\n",
    "noise = 1.5\n",
    "\n",
    "# Generate a random set of x and y (note, we fix x here, i.e. x is without \"error\"):\n",
    "# `collect` just turns 1:0.1:5 into a real array.\n",
    "x = collect(1.0:0.1:5.0)\n",
    "n = length(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create normal distributed noise (using randn, we need to scale with our noise level)\n",
    "ϵ = noise.*randn(n)\n",
    "\n",
    "# Compute y given x and add some random number (normal distribution) to it\n",
    "# anim = @animate  for i ∈ 1:20\n",
    "# ϵ = noise.*randn(n)\n",
    "\n",
    "# Construct noisy pseudo-measurements\n",
    "y = a1.+ a2.*x .+ ϵ\n",
    "# Define K matrix here (why is that so?)\n",
    "K = [ones(n) x]\n",
    "\n",
    "# Plot all dots\n",
    "p1 = plot(x, y, seriestype=:scatter,ylabel=\"y\", xlabel=\"x\", label=\"Data\", legend=:topleft )\n",
    "\n",
    "# Solve Normal Equation (see how easy this is in Julia?):\n",
    "a = inv(K'K)K'y\n",
    "\n",
    "println(\"Offset \", a[1])\n",
    "println(\"Slope \", a[2])\n",
    "\n",
    "# Reconstruct fit (if you write plot! it writes OVER what you had before, not creating something new):\n",
    "plot!(x, K*a, lw=2, label=\"Fit\")\n",
    "\n",
    "# Plot residuals\n",
    "p2 = plot(x, y-K*a, seriestype=:scatter, ylabel=\"y-Ka\", xlabel=\"x\", label=\"Residuals\")\n",
    " \n",
    "#end;\n",
    "# Plot command here has to be at the end to show up:\n",
    "plot(p1, p2,  layout = (2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gif(anim,  fps = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at noise normalized residuals:\n",
    "\n",
    "println(std(ϵ))\n",
    "println(std(y - K*a))\n",
    "histogram((y - K*a)./noise,bins=10,label=\"Residuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goodness of fit\n",
    "A bad correlation coefficient between $x$ and $y$ doesn't necessarily mean that there is no causation. A better metric for the goodness of fit is the $\\chi^2$ statistics, as our model-data mismatches should follow that statistic if they behave the way we expected. \n",
    "Of particular importance is the reduced $\\chi^2$, which we can define as\n",
    "$$\\chi_r^2 = \\frac{\\chi^2}{DOF}$$\n",
    "with the degrees of freedom DOF defined as the number of observations $n$ minus the number of fitted parameters. The $\\chi^2$ distribution represents the distribution of a sum of the squares of k independent standard normal random variables. This is exactly what we have if we take the sum of the squared residuals (actually the quantity that we minimize!).\n",
    "See https://en.wikipedia.org/wiki/Chi-square_distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular χ²:\n",
    "χ² = sum(((K*a.-y)./noise).^2)\n",
    "# reduced χ²:\n",
    "χ²_r = χ²/(n-length(a))\n",
    "println(χ²_r)\n",
    "#print(np.corrcoef(x, y)[0,1]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "\n",
    "Now let's try to do some same with a better knowledge of the individual measurement errors, which we can include in a measurement error covariance matrix $S_\\epsilon$. In the simplest case, $S_\\epsilon$ is purely diagonal as measurement errors are usually not correlated with each other (but could). We keep it in Matrix form here to remain general but use a diagonal form.\n",
    "\n",
    "\\begin{equation}\n",
    "S_{\\epsilon} = \\left( \\begin{array}{ccccc}\n",
    "\\sigma_1^2 & \\hfill & \\hfill & \\hfill & \\hfill \\\\\n",
    "\\hfill & \\sigma_2^2 & \\hfill & \\hfill & \\hfill \\\\\n",
    "\\hfill & \\hfill & \\ddots &\\hfill & \\hfill \\\\\n",
    "\\hfill & \\hfill & \\hfill & \\sigma_{n-1}^2 & \\hfill \\\\\n",
    "\\hfill & \\hfill & \\hfill & \\hfill & \\sigma_{n}^2 \\\\\n",
    "\\end{array} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "with the inverse $S^{-1}_{\\epsilon}$ for the diagonal case being:\n",
    "\\begin{equation}\n",
    "S^{-1}_{\\epsilon} = \\left( \\begin{array}{ccccc}\n",
    "1/\\sigma_1^2 & \\hfill & \\hfill & \\hfill & \\hfill \\\\\n",
    "\\hfill & 1/\\sigma_2^2 & \\hfill & \\hfill & \\hfill \\\\\n",
    "\\hfill & \\hfill & \\ddots &\\hfill & \\hfill \\\\\n",
    "\\hfill & \\hfill & \\hfill & 1/\\sigma_{n-1}^2 & \\hfill \\\\\n",
    "\\hfill & \\hfill & \\hfill & \\hfill & 1/\\sigma_{n}^2 \\\\\n",
    "\\end{array} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "What we need to derive now is the minimum of the weighted differences. In matrix form, the sum of the weighted squared difference can be written as $$(y-Ka)^T S^{-1}_{\\epsilon} (y-Ka)$$. Here, $S_{\\epsilon}^{-1}$ is now the weighting matrix $W$ that we used in the derivation of the normal equations (and it weighs each squared residual with the inverse of its variance, which is what we want!).\n",
    "\n",
    "Finding the zero slope in the derivate with respect to a thus yields a slightly modified equation to above, as derived in Lecture Video 3:\n",
    "$$a = (K^T S^{-1}_\\epsilon K)^{-1}K^T S^{-1}_\\epsilon y$$\n",
    "\n",
    "Compare this to the optimal solution found in Rodgers at page 25, equation 2.30. What is the main difference? \n",
    "_______________\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try solving the same problem as above again, this just using the knowledge that the $1\\sigma$ error the same as previously defined as \"noise\" level, a constant noise for each measurement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct error covariance matrix\n",
    "Sϵ = Diagonal((ones(length(ϵ)).*noise).^2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now solve the equation again as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# somewhat overkill here as invSe is diagonal but it works:\n",
    "aS = inv(K'inv(Sϵ)*K)K'inv(Sϵ)*y\n",
    "\n",
    "println(\"Offset \", aS[1])\n",
    "println(\"Slope \", aS[2])\n",
    "\n",
    "# Reconstruct fit:\n",
    "scatter(x,y, label=\"Data\") \n",
    "plot!(x, K*aS, label=\"Fit\",lw=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What use is this?\n",
    "______\n",
    "In this example, the 2 solutions are identical as the weighting is the same for each measurement (which can be changed). The biggest advantage in having an idea about the actual measurement uncertainty is that we can derive  uncertainties in the derived state vector $a$ (in the future, we will use $x$ for the state vector).\n",
    "The error covariance of the retrieved state vector is (which we kind of computed already before), see derivation in Rodgers, eq. 2.27, which includes prior knowledge of our state vector:\n",
    "\n",
    "$$\\hat{S}=(K^T S^{-1}_\\epsilon K)^{-1}$$\n",
    "\n",
    "What does $\\hat{S}$ represent? It shows how the retrieved state vector $\\hat{x}$ varies around the true value of $x$. This distribution is again normally distributed with the covariance matrix $\\hat{S}$.\n",
    "\n",
    "If the noise matrix is purely diagonal and all elements are identical, we can simplify this to \n",
    "$$\\hat{S}=\\chi^2_r (K^T K)^{-1}$$\n",
    "\n",
    "This has the advantage that we can pre-compute $(K^T K)^{-1}$ in many cases where this remains constant and can be applied to a multitude of problem sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior covariance matrix S\n",
    "Ŝ = inv(K'inv(Sϵ)K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______\n",
    "This might be something you have never really paid attention to as most standard methods never provide the full error covariance matrix (the 2x2 matrix here really is an easy case). \n",
    "What does the off-diagonal mean? The errors seems to correlated with the correlation coefficient $corr(a_1,a_2)=cov(a_1,a_2)/(std(a_1)\\cdot std(a_2))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Correlation between retrieved a1 and a2: \", Ŝ[1,2]/(sqrt(Ŝ[1,1]*sqrt(Ŝ[2,2]))))\n",
    "println(\"1σ error in a1: \", sqrt(Ŝ[1,1]))\n",
    "println(\"1σ error in a2: \", sqrt(Ŝ[2,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "<font color=red>**Question:**</font>  Any idea why these variable are correlated with each other (negatively!)? Can you think of a graphical explanation of it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________\n",
    "<font color=red>**Question:**</font> How could you re-phrase the problem to make the fits of $a_1$ and $a_2$ more independent of each other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "Let us run it again using an ensemble to look at the distribution of retrieved properties:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-compute Gain Matrix G as we don't need to do that all the time:\n",
    "@time Gain = inv(K'inv(Sϵ)K)K'*inv(Sϵ)\n",
    "\n",
    "# True values:\n",
    "a1 = 0.0\n",
    "a2 = 3.0\n",
    "\n",
    "# Number of samples:\n",
    "samples = 10000\n",
    "ar = zeros((samples,2))\n",
    "chi2_ = zeros((samples,))\n",
    "println(a1,a2)\n",
    "\n",
    "# Loop through different random fits\n",
    "for i=1:samples\n",
    "    y = a1.+a2.*x .+ noise.*randn(length(x))\n",
    "    a = Gain*y\n",
    "    ar[i,1]=a[1]\n",
    "    ar[i,2]=a[2]\n",
    "    # Save reduced chi2 as well:\n",
    "    chi2_[i]=sum(((K*a-y)./noise).^2)/(n-length(a))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(ar[:,1],ar[:,2],markersize=0.5, alpha=0.15, label=\"Fitted parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a tool for that \n",
    "cornerplot(ar, compact=true, label = [\"a$i\" for i=1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the covariance computed from the set of retrieved a_1 and a_2\n",
    "cov(ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ŝ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "Compare the \"data-derived\" distribution with the one we predicted using the error-covariance before. Due to the large sample size, they come pretty close. In our case, this was a well behaved example with Gaussian errors but this example also gives you an idea how to derive error statistics using monte carlo simulations (which could draw random errors from a non-Gaussian distribution for instance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dof = n-length(a)\n",
    "rv = Chisq(dof)\n",
    "xx = 0.35:0.02:2.5\n",
    "plot(xx,pdf.(rv,xx*dof)*dof,lw=3, alpha=0.5,  label=\"Theoretical\")\n",
    "histogram!(chi2_,t=:density,label=\"Measured\" )\n",
    "xlabel!(\"χ²_r\")\n",
    "#plt.legend()\n",
    "#print(dof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(xx,cdf.(rv,xx*dof),lw=3, alpha=0.5,  label=\"Theoretical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "<font color=red>**Question:**</font> \n",
    "How would you define suitable thresholds for $\\chi^2$ to filter out data that didn't match your expected noise behavior?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "<font color=red>**Question:**</font> \n",
    "How can you change the setup mentioned before to use different polynomial degrees? How would you force the fit to go through the origin? How would you use Legendre Polynomials or other functions instead of simple polynomials used here? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 1.5.2",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
